{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1n09orC7nof"
   },
   "source": [
    "# Using Media Pipe to detect hands and create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:43:57.151721Z",
     "start_time": "2021-12-06T03:43:55.514120Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v63pqybTGocC",
    "outputId": "c5ab264c-f7c3-4dfa-8ccf-57b092c95593"
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using drawing_utils and solutions.hands for detecting and outlining hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:43:57.162953Z",
     "start_time": "2021-12-06T03:43:57.154453Z"
    },
    "id": "_YU24X1vGuNM"
   },
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:43:57.178292Z",
     "start_time": "2021-12-06T03:43:57.168563Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DATA_DIR = 'data'\n",
    "KEYPOINTS_FNAME = 'keypoints'\n",
    "DIRECTION = {\n",
    "    'up': 0,\n",
    "    'down': 1,\n",
    "    'back': 2,\n",
    "    'forward': 3,\n",
    "    'left': 4,\n",
    "    'right': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbK4t3HSQ4Se"
   },
   "source": [
    "## Defining Keypoints\n",
    "\n",
    "Keypoints is used to store Mediapipe hand landmarks temporary before storing it in a keypoints.csv file\n",
    "\n",
    "![Landmarks by media pipe](https://google.github.io/mediapipe/images/mobile/hand_landmarks.png)\n",
    "\n",
    "> “Hands,” mediapipe. https://google.github.io/mediapipe/solutions/hands.html.\n",
    "\n",
    "\n",
    "Keypoints store data as [sample1, sample2, sample3,..., sampleN]<br />\n",
    "where<br />\n",
    "```\n",
    "sample1 = [label, x0, y0, x1, y1, ..., x21, y21] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:43:58.900690Z",
     "start_time": "2021-12-06T03:43:58.897135Z"
    }
   },
   "outputs": [],
   "source": [
    "keypoints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:43:59.324014Z",
     "start_time": "2021-12-06T03:43:59.315696Z"
    }
   },
   "outputs": [],
   "source": [
    "# landmark = results.multi_hand_landmarks[0]\n",
    "def extract_keypoints(landmark, direction):\n",
    "    if not landmark: return\n",
    "    temp_list = [DIRECTION[direction]]\n",
    "    for data_point in enumerate(landmark[0].landmark):\n",
    "        temp_list.extend(\n",
    "            [data_point[1].x, data_point[1].y]\n",
    "        )\n",
    "    \n",
    "    keypoints.append(temp_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating hand connection using add_hands_connection()\n",
    "\n",
    "### <p>Mediapipe works with RGB not BGR <br />\n",
    "\n",
    "```\n",
    "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "```\n",
    "</p>\n",
    "\n",
    "### Writeable = False\n",
    "<p>Turn off writeable so image is passed as reference into hands.process so it can be processed faster</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:44:00.110010Z",
     "start_time": "2021-12-06T03:44:00.094983Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_hands_connection(frame, hands):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, hand, \n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(121,22,78), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2),\n",
    "            )\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:44:01.028557Z",
     "start_time": "2021-12-06T03:44:01.020640Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_text_on_video(frame, index, number_img, direction):\n",
    "    index += 1\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame,\n",
    "               f\"{direction}: {index}/{number_img}\",\n",
    "               (50, 50),\n",
    "                font, 1, \n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T14:10:44.146324Z",
     "start_time": "2021-12-02T14:10:44.139802Z"
    }
   },
   "source": [
    "## Using solutions.hands.Hands\n",
    "\n",
    "### min_detection_confidence \n",
    "> threshold for initial hand detection to successful 0.8 means 80% chance of succefully detecting hand\n",
    "### min_tracking_confidence \n",
    "> threshold to track detected hands\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:44:02.922895Z",
     "start_time": "2021-12-06T03:44:02.908041Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "N1OBOosMGzTg",
    "outputId": "c46d8cf3-ee3e-4aad-b188-8932114d093f"
   },
   "outputs": [],
   "source": [
    "def creating_keypoint_data(number_img=10):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    for key in DIRECTION:\n",
    "        capturing_image_idx = 0\n",
    "        print(f\"Capturing for {key}\")\n",
    "        \n",
    "        last_recorded_time = time.time()\n",
    "        with mp_hands.Hands(min_detection_confidence=0.9, min_tracking_confidence=0.7) as hands:\n",
    "                while cap.isOpened():  \n",
    "                    if (capturing_image_idx >= number_img): break\n",
    "                    ret, frame = cap.read()\n",
    "                    image, results = add_hands_connection(frame, hands)\n",
    "                    image = cv2.flip(image, 1)\n",
    "                    write_text_on_video(image, \n",
    "                                    capturing_image_idx, \n",
    "                                    number_img, \n",
    "                                    key)\n",
    "                    cv2.imshow(\"Hand tacking\", image)\n",
    "                    curr_time = time.time()\n",
    "                    if (curr_time - last_recorded_time) >= 3:\n",
    "                        capturing_image_idx += 1\n",
    "                        print(f\" \\\n",
    "                              \\tcapturing image ->>>> \\\n",
    "                              {capturing_image_idx} of {key}\\n\")\n",
    "                        landmark_result = results.multi_hand_landmarks\n",
    "                        extract_keypoints(landmark_result, key) \n",
    "                        last_recorded_time = curr_time\n",
    "                        \n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:44:12.633514Z",
     "start_time": "2021-12-06T03:44:03.595764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing for up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               \tcapturing image ->>>>                               1 of up\n",
      "\n",
      "Capturing for down\n",
      "Capturing for back\n",
      "Capturing for forward\n",
      "Capturing for left\n",
      "Capturing for right\n"
     ]
    }
   ],
   "source": [
    "creating_keypoint_data(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T11:24:40.719337Z",
     "start_time": "2021-12-03T11:24:40.715801Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T11:24:41.884714Z",
     "start_time": "2021-12-03T11:24:41.866299Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(OUTPUT_DATA_DIR):\n",
    "    os.mkdir(OUTPUT_DATA_DIR)\n",
    "    \n",
    "path_to_csv = os.path.join(OUTPUT_DATA_DIR, f\"{KEYPOINTS_FNAME}_{unique_id}.csv\")    \n",
    "with open(path_to_csv, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai_hand_pose_detection_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
